# Hey this is my network security project ## 
## lets goooooooooooooooooooooooooooooooo

# Data Ingestion â†’ Data Validation â†’ Data Transformation â†’ Training â†’ Evaluation â†’ Deployment



# ğŸ“¥ Data Ingestion Module

The Data Ingestion module is responsible for extracting raw network security data from MongoDB, storing it as feature-store data, and preparing clean train/test datasets for downstream pipeline stages.

This is the first step in the ML pipeline.

ğŸ¯ Objectives

This module:

âœ” Connects securely to MongoDB
âœ” Loads data into a pandas DataFrame
âœ” Cleans system-generated _id fields
âœ” Stores a local feature-store copy
âœ” Splits data into train & test sets
âœ” Saves outputs as versioned artifacts

It ensures every pipeline run uses consistent, reproducible data.

ğŸ§  Class: DataIngestion
Inputs

Provided via DataIngestionConfig:

Setting	Purpose
database_name	MongoDB database
collection_name	MongoDB collection
feature_store_file_path	Path to store raw dataset
training_file_path	Path for training dataset
testing_file_path	Path for testing dataset
train_test_split_ratio	% test size

MongoDB connection is read from:

MONGO_DB_URL (env variable)

ğŸ“¤ Outputs

Returns a DataIngestionArtifacts object:

Field	Description
trained_file_path	Path to training CSV
test_file_path	Path to testing CSV

These files are used by Data Validation next.

âš™ï¸ Processing Workflow
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Connect to MongoDB           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load Collection into DataFrameâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Remove _id Column            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Save Raw Data (Feature Store)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Trainâ€“Test Split             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Save Train & Test Files      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§ª Key Features
1ï¸âƒ£ Extract Data from MongoDB

The module connects to MongoDB and loads the full dataset into a pandas DataFrame:

âœ” reads all records
âœ” converts to DataFrame
âœ” drops _id column
âœ” replaces "na" with NaN

This ensures ML-ready data.

2ï¸âƒ£ Feature Store Export

The raw dataset is stored locally as:

feature_store_file_path


Purpose:

âœ” reproducibility
âœ” offline access
âœ” audit trail

3ï¸âƒ£ Trainâ€“Test Split

Performed using sklearn.model_selection.train_test_split:

Reproducible via random_state=42

Test size configurable

Train & Test saved separately

ğŸ“ Artifacts Created

Example structure:

Artifacts/
 â””â”€â”€ <timestamp>/
     â”œâ”€â”€ data_ingestion/
     â”‚   â”œâ”€â”€ feature_store.csv
     â”‚   â”œâ”€â”€ train.csv
     â”‚   â””â”€â”€ test.csv

ğŸš¨ Error Handling

All failures are wrapped into:

NetworkSecurityException


Ensuring:

âœ” clear stack trace
âœ” consistent error logging
âœ” graceful pipeline failure

ğŸ¯ Why Data Ingestion Matters

This step guarantees that:

âœ” data is versioned
âœ” sources are traceable
âœ” splits are consistent
âœ” pipeline remains stable

It sets a strong foundation for the ML workflow.



# ğŸ§© Data Validation Module â€” Overview

This module validates the ingested training and test datasets before they enter the ML pipeline. The goal is to ensure the data schema is correct and to detect potential data drift between training and testing splits.

âœ” Key Responsibilities
1ï¸âƒ£ Load Schema Configuration

Reads the schema YAML file

Uses it to validate:

Expected columns

Numerical column count

2ï¸âƒ£ Read Input CSV Files

Utility method:

read_data(path)


Loads CSV files into pandas DataFrames.

3ï¸âƒ£ Validate Schema
âœ” Check column count

Verifies that the dataset contains the exact number of expected columns.

âœ” Validate numerical columns

Confirms the number of numeric-type columns matches what the schema defines.

If any mismatch occurs â†’ validation fails.

4ï¸âƒ£ Detect Data Drift (KS Test)

Uses Kolmogorovâ€“Smirnov test (ks_2samp) to compare:

train distribution  vs  test distribution


for each feature.

If p-value â‰¥ 0.05 â†’ no drift

If p-value < 0.05 â†’ drift detected

A YAML report is created and stored at:

drift_report_file_path

5ï¸âƒ£ Save Validated Datasets

If validation succeeds:

Train â†’ valid_train_file_path

Test â†’ valid_test_file_path

Both are saved in CSV format.

6ï¸âƒ£ Return Validation Artifact

Builds and returns a DataValidationArtifacts object containing:

Field	Meaning
validaion_status	Whether data passed validation
valid_train_file_path	Path to cleaned train data
valid_test_file_path	Path to cleaned test data
invalid_train_file_path	(Reserved for future use)
invalid_test_file_path	(Reserved for future use)
drift_report_file_path	YAML drift report

This artifact is then used by the next pipeline stage.

ğŸš¨ Error Handling

Any exception is wrapped and raised as:

NetworkSecurityException


So the pipeline stops safely with meaningful logs.

ğŸ¯ Why This Step Matters

Data validation prevents:

âœ” training on corrupted data
âœ” schema mismatch crashes
âœ” silent performance degradation due to drift

It keeps the ML pipeline reliable and reproducible.





